{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import train_top\n",
    "import training_pipeline.training_pipeline as training_pipeline\n",
    "import bridge.tokens\n",
    "from mubert import mubert, modeling, toptokens\n",
    "from jargon.bert import tokenization as jargon_tokenization\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"bridgebot_config_small.json\"\n",
    "checkpoint = \"\"\n",
    "bert_vocab_file = \"gs://njt-serene-epsilon/jargon/uncased_L-8_H-512_A-8/vocab.txt\"\n",
    "data_file = \"gs://njt-serene-epsilon/top-dataset-lin-bids/top_builder/bridgebot-top/0.0.6/top_builder-test.tfrecord-00000-of-00001\"\n",
    "init_checkpoint = \"gs://njt-serene-epsilon/models/top.small.0/lin-bids/model.ckpt-362000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.compat.v1.flags\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_player_seq_length\", 256,\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded. Must match data generation.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_jargon_seq_length\", 256,\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded. Must match data generation.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_top_seq_length\", 16,\n",
    "    \"Must match data generation.\")\n",
    "\n",
    "flags.DEFINE_string(\"jargon_vocab_file\", bert_vocab_file,\n",
    "    \"The vocabulary file that the jargon/BERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_bool(\"jargon_do_lower_case\", True, \"True if model is uncased.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_player_sequence_out\", False, \"Use only embedding if False.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_jargon_sequence_out\", False, \"Use only embedding if False\")\n",
    "\n",
    "flags.DEFINE_bool(\"freeze_player\", True, \"Stop player gradient if True\")\n",
    "\n",
    "flags.DEFINE_bool(\"freeze_jargon\", True, \"Stop jargon gradient if True\")\n",
    "\n",
    "flags.DEFINE_string(\"f\", '', \"hack\") # seem required for flags to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = training_pipeline.TopConfig(name='attention_data',\n",
    "        max_player_seq_length=FLAGS.max_player_seq_length,\n",
    "        max_jargon_seq_length=FLAGS.max_jargon_seq_length,\n",
    "        max_top_seq_length=FLAGS.max_top_seq_length)\n",
    "\n",
    "def _ex_feature_info(fi):\n",
    "    if fi.dtype == tf.int32:\n",
    "        return tf.io.FixedLenFeature(fi.shape, tf.int64)\n",
    "    else:\n",
    "        return tf.io.FixedLenFeature(fi.shape, fi.dtype)\n",
    "\n",
    "features_info = {k: _ex_feature_info(v) for k,v in\n",
    "                    training_pipeline._top_features_info(data_config).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(data_config):\n",
    "    # Hack to make iterating possible without eager execution\n",
    "    dataset = tfds.as_numpy(tf.data.TFRecordDataset(data_file))\n",
    "    for rec in dataset:\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            example = sess.run(tf.io.parse_single_example(rec, features_info))\n",
    "        yield example\n",
    "            \n",
    "def model_builder(bridgebot_config, init_checkpoint):\n",
    "    def model_fn(features, params):        \n",
    "        predictions, top_embedding_table, top_attention_probs =\\\n",
    "            train_top.features_to_predictions(features,\n",
    "                                    params,\n",
    "                                    bridgebot_config,\n",
    "                                    use_one_hot_embeddings=False,\n",
    "                                    is_training=False,\n",
    "                                    compute_attention_probs=True)\n",
    "        \n",
    "        target_ids = features[\"target_ids\"]\n",
    "        target_positions = features[\"target_positions\"]\n",
    "        target_weights = features[\"target_weights\"]\n",
    "\n",
    "        with tf.compat.v1.variable_scope(\"top\"):\n",
    "            top_loss, top_example_loss, top_log_probs = \\\n",
    "                mubert.get_masked_output(\n",
    "                    bridgebot_config.top, predictions, \n",
    "                    top_embedding_table,\n",
    "                     target_positions, target_ids, target_weights)\n",
    "        \n",
    "        train_top.init_from_checkpoint(init_checkpoint, False)\n",
    "        \n",
    "        return top_log_probs, top_attention_probs\n",
    "    return model_fn\n",
    "    \n",
    "def get_initialized_model(bridgebot_config):\n",
    "    with tf.name_scope(\"attention\") as scope:\n",
    "        model = model_builder(bridgebot_config, init_checkpoint)\n",
    "    return model\n",
    "        \n",
    "def run_example(bridgebot_config, features_gen):\n",
    "\n",
    "    def _to32(dtype):\n",
    "        if dtype == tf.int64:\n",
    "            return tf.int32\n",
    "        return dtype\n",
    "\n",
    "    first_run = True\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        for example in features_gen:\n",
    "            features = {k: tf.constant(v, shape=(1, len(v)), \n",
    "                        dtype=_to32(features_info[k].dtype))\n",
    "                        for k, v in example.items()}\n",
    "            \n",
    "            model = get_initialized_model(bridgebot_config)\n",
    "            params = {\"batch_size\" : 1}\n",
    "            top_log_probs, top_attention_probs = model(features, params)\n",
    "            if first_run:\n",
    "                sess.run(tf.compat.v1.global_variables_initializer())\n",
    "                first_run = False\n",
    "            yield sess.run((top_attention_probs, features))\n",
    "\n",
    "def features_to_view(bridgebot_config, features):\n",
    "    player_tokenizer = bridge.tokens.Tokenizer()\n",
    "    jargon_tokenizer = jargon_tokenization.FullTokenizer(\n",
    "        vocab_file=FLAGS.jargon_vocab_file,\n",
    "        do_lower_case=FLAGS.jargon_do_lower_case)\n",
    "    top_tokenizer = toptokens.Tokenizer()\n",
    "    \n",
    "    # batch size = 1\n",
    "    player_input_ids = features[\"player_input_ids\"][0]\n",
    "    jargon_input_ids = features[\"jargon_input_ids\"][0]\n",
    "    query_ids = features[\"query_ids\"][0]\n",
    "    \n",
    "    hidden_pad = [\"[HIDDEN]\"] *\\\n",
    "        bridgebot_config.mubert.representation.hidden_state_length\n",
    "    player_tokens = player_tokenizer.ids_to_tokens(player_input_ids)\n",
    "    jargon_tokens = jargon_tokenizer.convert_ids_to_tokens(jargon_input_ids)\n",
    "    query_tokens = top_tokenizer.ids_to_tokens(query_ids)\n",
    "    \n",
    "    return hidden_pad + player_tokens + jargon_tokens + query_tokens\n",
    "\n",
    "\n",
    "def show_head_view(bridgebot_config, features, attention):\n",
    "    attention = [torch.from_numpy(layer) for layer in attention]\n",
    "    tokens = features_to_view(bridgebot_config, features)   \n",
    "    head_view(attention, tokens)\n",
    "\n",
    "    \n",
    "def tokens_to_squeeze(tokens, accepted=('[HIDDEN]', '[PAD]')):\n",
    "    prev_t,  start_idx = None, None\n",
    "    for i, t in enumerate(tokens + [None]):\n",
    "        emit = True\n",
    "        if t is not None:\n",
    "            if t.startswith('##'):\n",
    "                emit = False\n",
    "            elif t in accepted:\n",
    "                if t == prev_t:\n",
    "                    emit = False\n",
    "        if emit:\n",
    "            if start_idx is not None:\n",
    "                if i == start_idx + 1:\n",
    "                    repl = prev_t\n",
    "                elif prev_t.startswith('##'):\n",
    "                    repl = \"\".join([tokens[start_idx]] + \\\n",
    "                         [t[2:] for t in tokens[start_idx + 1:i]])\n",
    "                else:\n",
    "                    repl = \"{} * {}\".format(i - start_idx, tokens[start_idx])\n",
    "                yield slice(start_idx, i), repl\n",
    "            start_idx = i\n",
    "        prev_t = t\n",
    "        \n",
    "def squeezed_tokens_and_attention(tokens, attention):\n",
    "    squeeze_info = list(tokens_to_squeeze(tokens))\n",
    "    squeezed_tokens = [t for _, t in squeeze_info]\n",
    "    atten_shape = attention[0].shape\n",
    "    target_size = len(squeeze_info)\n",
    "    squeezed_attention = []\n",
    "    for layer in attention:\n",
    "        new_shape = list(atten_shape)\n",
    "        new_shape[2] = target_size\n",
    "        row_squeeze = np.zeros(new_shape)\n",
    "        for i, (s, _) in enumerate(squeeze_info):\n",
    "            row_squeeze[:,:,i,:] = np.sum(layer[:,:,s,:], axis=2)\n",
    "        new_shape[3] = target_size\n",
    "        col_squeeze = np.zeros(new_shape)\n",
    "        for i, (s, _) in enumerate(squeeze_info):\n",
    "            col_squeeze[:,:,:,i] = np.mean(row_squeeze[:,:,:,s], axis=3)\n",
    "        squeezed_attention.append(col_squeeze)\n",
    "    return squeezed_tokens, squeezed_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "bridgebot_config = modeling.BridgebotConfig.from_json_file(config_file)\n",
    "features_gen = get_examples(data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_gen = run_example(bridgebot_config, features_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "atten, ex = next(attention_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = features_to_view(bridgebot_config, ex)\n",
    "sq_tokens, sq_atten = squeezed_tokens_and_attention(all_tokens, atten)\n",
    "all_torch_atten = [torch.from_numpy(10*layer) for layer in sq_atten]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_view(all_torch_atten, sq_tokens, sentence_b_start=sq_tokens.index('[CLS]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-still",
   "metadata": {},
   "source": [
    " all_tokens[177:181] + all_tokens[135:139]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = np.zeros(len(all_tokens), dtype=bool)\n",
    "sel[177:181] = 1\n",
    "sel[390:394] = 1\n",
    "sel[414:417] = 1\n",
    "sel[640:644] = 1\n",
    "sel[384:388] = 1\n",
    "sel[403:407] = 1\n",
    "np.array(all_tokens)[sel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-wrapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens[384:388]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens[403:407]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-blame",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_atten(idxs, tokens, atten):\n",
    "    t = np.array(atten)[:,:,:,idxs,:]\n",
    "    return np.array(tokens)[idxs], t[:,:,:,:, idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-rolling",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_tokens, sel_atten = select_atten(sel, all_tokens, atten)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "invisible-operations",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "featured-opposition",
   "metadata": {},
   "source": [
    "sel_atten_torch = [torch.from_numpy(sel_atten[i]) for i in range(len(sel_atten))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_atten_torch = torch.from_numpy(sel_atten)\n",
    "head_view(sel_atten_torch, sel_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sel_atten[:,:,:,:,:].sum(axis=(0,1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-moisture",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
